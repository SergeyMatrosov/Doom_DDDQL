{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "game = DoomGame()\n",
    "#YOUR PATH TO DEATHMATCH CONFIG/ViZDoom-master/scenarios/deathmatch.cfg\n",
    "#game.load_config(\"vizdoom/scenarios/deathmatch.cfg\")\n",
    "game.load_config(\"YOUR PATH/ViZDoom-master/scenarios/basic.cfg\")\n",
    "game.set_screen_resolution(ScreenResolution.RES_1280X1024)\n",
    "game.get_available_buttons()\n",
    "\n",
    "buttons = game.get_available_buttons()\n",
    "print(buttons)\n",
    "actions = []\n",
    "actions_dict = {}\n",
    "for i in range(buttons.index(Button.ATTACK)+1):\n",
    "    act_name = str(buttons[i])[7:]\n",
    "    act = []\n",
    "    for k in range(buttons.index(Button.ATTACK)+1):\n",
    "        act.append(0)\n",
    "    act[i] = 1\n",
    "    actions.append(act)\n",
    "    actions_dict[str(act_name)+'-'+str(i+1)]=act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        self.explore_start = 1.0\n",
    "        self.explore_stop = 0.01\n",
    "        self.decay_rate = 0.0001\n",
    "    \n",
    "        self.state = self.get_state()\n",
    "        self.actions = self.get_actions()\n",
    "        self.impressions = 0\n",
    "        self.stack_size = 3\n",
    "        \n",
    "        self.tensorflowboard = 'YOUR PATH TO LOG FILES'\n",
    "        \n",
    "        #memory\n",
    "        #by default - 100000 cell \n",
    "        #will be 4 cells in each cell -> experience tuple()\n",
    "        self.memory = self.Memory(capacity=100000)\n",
    "        self.frame = self.make_frame()\n",
    "        \n",
    "        #models: for Q and target Q\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "           \n",
    "    def get_state(self, state=np.zeros((3, 1024, 1280), dtype='uint8')):\n",
    "        state = state\n",
    "        return state\n",
    "                          #attack\n",
    "                          #for another game\n",
    "                          #actions=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                           \n",
    "    def get_actions(self, actions=[1, 0, 0]):\n",
    "        actions = actions\n",
    "        return actions\n",
    "    \n",
    "    def act(self, state, actions=actions):\n",
    "        if np.random.rand() <= self.explore_start:\n",
    "            action = random.choice(actions)\n",
    "            return action\n",
    "        values_of_actions = self.model.predict(state)\n",
    "        action = actions[np.argmax(values_of_actions)]\n",
    "        return action  \n",
    "    \n",
    "    def make_frame(self, state=np.zeros((3, 1024, 1280), dtype='uint8')):\n",
    "        #вычитание фреймов друг из друга передает динамику (velocity)\n",
    "        dymanic_frame = np.array(state[0]) - np.array(state[1])\n",
    "        gray_frame = rgb2gray(dymanic_frame)\n",
    "        cropped_frame = dymanic_frame[150:, :]\n",
    "        resized_frame = transform.resize(cropped_frame, [500, 500])\n",
    "        frame = np.reshape(resized_frame, (1, 500, 500, 1))\n",
    "        return frame\n",
    "    \n",
    "    def get_impressions(self):\n",
    "        #64 refers to maximum backet\n",
    "        if(self.impressions) < 64:\n",
    "            impressions = self.impressions + 1\n",
    "            return impressions\n",
    "        return self.impressions\n",
    "        \n",
    "    def experience(self, frame, action, reward, next_frame, done):\n",
    "        experience = (frame, action, reward, next_frame, done)\n",
    "        self.memory.store(experience)\n",
    "        \n",
    "    #for main and fix-target networks\n",
    "    \n",
    "    def build_model(self):\n",
    "        print('Print after training: tensorboard --logdir={}'.format(self.tensorflowboard))\n",
    "        #delete non-actual models in tensorflowboard (because every new object of Agent() gets NEW model)\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        input_layer = tf.keras.Input(shape=[500, 500, 1])\n",
    "        conv2D_1 = tf.keras.layers.Conv2D(\n",
    "                                         #input_shape=(500, 500, 1),\n",
    "                                         data_format='channels_last',\n",
    "                                         filters=32, \n",
    "                                         kernel_size=[60, 60],\n",
    "                                         strides = (4, 4),\n",
    "                                         padding = 'valid',\n",
    "                                         activation='tanh')(input_layer)\n",
    "        batchNormalization_1 = tf.keras.layers.BatchNormalization(epsilon=0.0001)(conv2D_1)\n",
    "        conv2D_2 = tf.keras.layers.Conv2D(filters=64, \n",
    "                                         kernel_size=[40, 40],\n",
    "                                         strides = (4, 4),\n",
    "                                         padding = 'valid',\n",
    "                                         activation='tanh')(batchNormalization_1)\n",
    "        batchNormalization_2 = tf.keras.layers.BatchNormalization(epsilon=0.0001)(conv2D_2)\n",
    "        conv2D_3 = tf.keras.layers.Conv2D(filters=128, \n",
    "                                         kernel_size=[10, 10],\n",
    "                                         strides = (4, 4),\n",
    "                                         padding = 'valid',\n",
    "                                         activation='tanh')(batchNormalization_2)\n",
    "        batchNormalization_3 = tf.keras.layers.BatchNormalization(epsilon=0.0001)(conv2D_3)\n",
    "        flatten = tf.keras.layers.Flatten()(batchNormalization_3)\n",
    "        \n",
    "        #Here we separate into two streams\n",
    "        #The one that calculate V(s)\n",
    "        value_pr = tf.keras.layers.Dense(700, activation='tanh')(flatten)\n",
    "        value = tf.keras.layers.Dense(1, activation=None)(value_pr)\n",
    "        \n",
    "        #The one that calculate A(s,a)\n",
    "        advantage_pr = tf.keras.layers.Dense(700, activation='tanh')(flatten)\n",
    "        advantage = tf.keras.layers.Dense(units=len(self.actions), activation=None)(advantage_pr)\n",
    "        # Aggregating layer\n",
    "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "        added = tf.keras.layers.Add()([value, tf.math.subtract(advantage, tf.math.reduce_mean(advantage, axis=1, keepdims=True))])\n",
    "        output_layer = tf.keras.layers.Dense(len(self.actions), activation=None)(added)\n",
    "        model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "        model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse',\n",
    "                      metrics=['accuracy', 'mean_squared_error'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print('Weights of TargetNet updated')\n",
    "    \n",
    "    def replay(self, batch_size=1):\n",
    "        tree_idx, batches, ISWeights_mb = self.memory.sample(batch_size)\n",
    "        states = []\n",
    "        target_Qs_batch = []    \n",
    "        \n",
    "        for batch in batches:\n",
    "            for state, action, reward, next_state, done in batch:\n",
    "                #this target will be only it was frame before end.\n",
    "                target = reward\n",
    "                print('Reward: ' + str(target))\n",
    "                if not done:\n",
    "                        #Double DNN learning aglorithm\n",
    "                        #Main Network predict actions for next state for selection of action\n",
    "                        target_by_MainNet = self.model.predict(next_state)[0]\n",
    "                        #index of best action\n",
    "                        Next_action_by_MainNet = np.argmax(target_by_MainNet)\n",
    "                        #Target Network predict next state to evalute actions and take action, \n",
    "                        #which was predicted by Main\n",
    "                        target_by_TargetNet = self.target_model.predict(next_state)[0]\n",
    "                        target = (reward + self.gamma * target_by_TargetNet[Next_action_by_MainNet])\n",
    "                target_f = agent.model.predict(state)\n",
    "                action_that_was_made = agent.actions.index(action)\n",
    "                predicted_value = target_f[0][action_that_was_made]\n",
    "\n",
    "                target_f[0][action_that_was_made] = target\n",
    "                actual_value = target_f[0][action_that_was_made]\n",
    "\n",
    "                loss = actual_value - predicted_value\n",
    "                print('Loss is: ' + str(loss))\n",
    "\n",
    "                #x and y for model.fit\n",
    "                states.append(state)\n",
    "                target_Qs_batch.append(target_f[0])\n",
    "        #make list to numpy array for model; reshape states to pass to model.fit\n",
    "        states = np.array([each for each in states])\n",
    "        states = np.reshape(states, (len(states), 500, 500, 1))\n",
    "        target_Qs_batch = np.array([each for each in target_Qs_batch])\n",
    "        print('Learning!')\n",
    "        \n",
    "        #logs to tensorflowboard\n",
    "        log_dir= self.tensorflowboard + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        #model train\n",
    "        self.model.fit(states, target_Qs_batch, batch_size=batch_size, epochs=1, verbose=0, callbacks=[tensorboard_callback])\n",
    "            #print(self.model.evaluate(state, target_f, verbose=0)[0])\n",
    "        print('End of learning')\n",
    "            \n",
    "    def save_network(self, path='/Users/s.matrosov/Downloads/Doom_Model'):\n",
    "        # Saves model at specified path as h5 file\n",
    "        #tf.keras.backend.clear_session()\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path='/Users/s.matrosov/Downloads/Doom_Model/'):\n",
    "        #tf.keras.backend.clear_session()\n",
    "        self.model = tf.keras.models.load_model(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "            \n",
    "    def increase_explotation(self, decay_step = 1):\n",
    "        explore_start = self.explore_stop + (self.explore_start - self.explore_stop) * np.exp(-self.decay_rate * decay_step)\n",
    "        return explore_start\n",
    "    \n",
    "    \n",
    "    #Memory, based on SumTree\n",
    "    class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "        \"\"\"\n",
    "        This SumTree code is modified version and the original code is from:\n",
    "        https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "        \"\"\"\n",
    "        PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "        PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "        PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "        PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "        absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "        def __init__(self, capacity):\n",
    "            # Making the tree \n",
    "            \"\"\"\n",
    "            Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "            And also a data array\n",
    "            We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "            We prefer to use a simple array and to overwrite when the memory is full.\n",
    "            \"\"\"\n",
    "            self.tree = self.SumTree(capacity)\n",
    "\n",
    "        \"\"\"\n",
    "        Store a new experience in our tree\n",
    "        Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "        \"\"\"\n",
    "        def store(self, experience):\n",
    "            # Find the max priority\n",
    "            max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "            # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "            # So we use a minimum priority\n",
    "            if max_priority == 0:\n",
    "                max_priority = self.absolute_error_upper\n",
    "\n",
    "            self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "        - Then a value is uniformly sampled from each range\n",
    "        - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "        - Then, we calculate IS weights for each minibatch element\n",
    "        \"\"\"\n",
    "        def sample(self, n):\n",
    "            # Create a sample array that will contains the minibatch\n",
    "            memory_batch = []\n",
    "                             #1D array with 1 row of n random numbers; #2D array with 1 columns and n rows\n",
    "            batch_idx, batch_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "            # Calculate the priority segment\n",
    "            # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "            priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "            # Here we increasing the PER_b each time we sample a new minibatch\n",
    "            self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "\n",
    "            # Calculating the max_weight\n",
    "            p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "            max_weight = (p_min * n) ** (-self.PER_b)\n",
    "\n",
    "            for i in range(n):\n",
    "                \"\"\"\n",
    "                A value is uniformly sample from each range\n",
    "                \"\"\"\n",
    "                low, high = priority_segment * i, priority_segment * (i + 1)\n",
    "                value = np.random.uniform(low, high)\n",
    "\n",
    "                \"\"\"\n",
    "                Experience that correspond to each value is retrieved\n",
    "                \"\"\"\n",
    "                index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "                #P(j)\n",
    "                sampling_probabilities = priority / self.tree.total_priority\n",
    "\n",
    "                #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "                batch_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "\n",
    "                batch_idx[i]= index\n",
    "\n",
    "                experience = [data]\n",
    "\n",
    "                memory_batch.append(experience)\n",
    "\n",
    "            return batch_idx, memory_batch, batch_ISWeights\n",
    "\n",
    "        \"\"\"\n",
    "        Update the priorities on the tree\n",
    "        \"\"\"\n",
    "        def batch_update(self, tree_idx, abs_errors):\n",
    "            abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "            clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "            ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "            for ti, p in zip(tree_idx, ps):\n",
    "                self.tree.update(ti, p)\n",
    "                \n",
    "        class SumTree(object):\n",
    "            data_pointer = 0\n",
    "            \"\"\"\n",
    "            Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "            \"\"\"\n",
    "            def __init__(self, capacity):\n",
    "                # Number of leaf nodes (final nodes) that contains experiences\n",
    "                self.capacity = capacity \n",
    "                # Generate the tree with all nodes values = 0\n",
    "                # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "                # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "                # Parent nodes = capacity - 1\n",
    "                # Leaf nodes = capacity\n",
    "                self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "                \"\"\" tree:\n",
    "                    0\n",
    "                   / \\\n",
    "                  0   0\n",
    "                 / \\ / \\\n",
    "                0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "                \"\"\"\n",
    "\n",
    "                # Contains the experiences (so the size of data is capacity)\n",
    "                self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "            \"\"\"\n",
    "            def add(self, priority, data):\n",
    "                # Look at what index we want to put the experience\n",
    "                tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "                \"\"\" tree:\n",
    "                    0\n",
    "                   / \\\n",
    "                  0   0\n",
    "                 / \\ / \\\n",
    "        tree_index  0 0  0  We fill the leaves from left to right\n",
    "                \"\"\"\n",
    "\n",
    "                # Update data frame\n",
    "                self.data[self.data_pointer] = data\n",
    "\n",
    "                # Update the leaf\n",
    "                self.update (tree_index, priority)\n",
    "\n",
    "                # Add 1 to data_pointer\n",
    "                self.data_pointer += 1\n",
    "\n",
    "                if self.data_pointer >= self.capacity: \n",
    "                    self.data_pointer = 0\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Update the leaf priority score and propagate the change through tree\n",
    "            \"\"\"\n",
    "            def update(self, tree_index, priority):\n",
    "                # Change = new priority score - former priority score\n",
    "                change = priority - self.tree[tree_index]\n",
    "                self.tree[tree_index] = priority\n",
    "\n",
    "                # then propagate the change through tree\n",
    "                while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "\n",
    "                    \"\"\"\n",
    "                    Here we want to access the line above\n",
    "                    THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "\n",
    "                        0\n",
    "                       / \\\n",
    "                      1   2\n",
    "                     / \\ / \\\n",
    "                    3  4 5  [6] \n",
    "\n",
    "                    If we are in leaf at index 6, we updated the priority score\n",
    "                    We need then to update index 2 node\n",
    "                    So tree_index = (tree_index - 1) // 2\n",
    "                    tree_index = (6-1)//2\n",
    "                    tree_index = 2 (because // round the result)\n",
    "                    \"\"\"\n",
    "                    tree_index = (tree_index - 1) // 2\n",
    "                    self.tree[tree_index] += change\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "            \"\"\"\n",
    "            def get_leaf(self, v):\n",
    "                \"\"\"\n",
    "                Tree structure and array storage:\n",
    "                Tree index:\n",
    "                     0         -> storing priority sum\n",
    "                    / \\\n",
    "                  1     2\n",
    "                 / \\   / \\\n",
    "                3   4 5   6    -> storing priority for experiences\n",
    "                Array type for storing:\n",
    "                [0,1,2,3,4,5,6]\n",
    "                \"\"\"\n",
    "                parent_index = 0\n",
    "\n",
    "                while True: # the while loop is faster than the method in the reference code\n",
    "                    left_child_index = 2 * parent_index + 1\n",
    "                    right_child_index = left_child_index + 1\n",
    "\n",
    "                    # If we reach bottom, end the search\n",
    "                    if left_child_index >= len(self.tree):\n",
    "                        leaf_index = parent_index\n",
    "                        break\n",
    "\n",
    "                    else: # downward search, always search for a higher priority node\n",
    "\n",
    "                        if v <= self.tree[left_child_index]:\n",
    "                            parent_index = left_child_index\n",
    "\n",
    "                        else:\n",
    "                            v -= self.tree[left_child_index]\n",
    "                            parent_index = right_child_index\n",
    "\n",
    "                data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "                return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "            @property\n",
    "            def total_priority(self):\n",
    "                return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'YOUR PATH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "#avaible_actions\n",
    "agent.actions = agent.get_actions(actions)\n",
    "\n",
    "#build new model\n",
    "agent.build_model()\n",
    "agent.save_network(path)\n",
    "\n",
    "#load existing model\n",
    "#agent.load_network(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.save_network(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.explore_start = 0.2\n",
    "\n",
    "episodes = 1500\n",
    "max_steps = 100 \n",
    "\n",
    "game.init()\n",
    "total_episode_rewards = []\n",
    "timestamps = []\n",
    "timestamp = 1\n",
    "\n",
    "for i in range(episodes):\n",
    "    print('Episode ' + str(i))\n",
    "    print('Exploration rate: {}'.format(agent.explore_start))\n",
    "    game.new_episode()\n",
    "    \n",
    "    finished = game.is_episode_finished()\n",
    "    if finished:\n",
    "        print('Finished? - ' + str(finished))\n",
    "        game.close()\n",
    "        game.init()\n",
    "        game.new_episode()\n",
    "    \n",
    "    episode_rewards = []\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        #стратовый фрэйм\n",
    "        if game.is_new_episode():\n",
    "            #получайем стек фреймов\n",
    "            state = game.get_state().screen_buffer\n",
    "            test_state = state\n",
    "            #агент все это превращает в фрейм вида: фрейм1 минус фрейм2 минус фрейм3\n",
    "            agent.frame = agent.make_frame(state)\n",
    "            #теперь состояние - это и есть этот фрейм\n",
    "            state = agent.frame\n",
    "            \n",
    "            agent.impressions = agent.get_impressions()\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            next_state = game.get_state().screen_buffer\n",
    "            agent.frame = agent.make_frame(next_state)\n",
    "            next_state = agent.frame\n",
    "            \n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            agent.experience(state, action, reward, next_state, done)\n",
    "            step = step + 1\n",
    "            time.sleep(0.02)\n",
    "            \n",
    "        #все остальные фреймы и до конца\n",
    "        else:\n",
    "            state = game.get_state().screen_buffer\n",
    "            agent.frame = agent.make_frame(state)\n",
    "            state = agent.frame\n",
    "            \n",
    "            #для бакета\n",
    "            agent.impressions = agent.get_impressions()\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                step = max_steps\n",
    "                #black screen\n",
    "                next_state = np.zeros((3, 1024, 1280), dtype='uint8')\n",
    "                agent.frame = agent.make_frame(next_state)\n",
    "                next_state = agent.frame\n",
    "                \n",
    "                agent.experience(state, action, reward, next_state, done)    \n",
    "                \n",
    "                agent.explore_start = agent.increase_explotation()\n",
    "                \n",
    "                time.sleep(0.02)\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                agent.frame = agent.make_frame(next_state)\n",
    "                next_state = agent.frame\n",
    "                \n",
    "                agent.experience(state, action, reward, next_state, done)\n",
    "                agent.explore_start = agent.increase_explotation()\n",
    "                \n",
    "                step = step + 1\n",
    "                time.sleep(0.02) \n",
    "    #training           \n",
    "    agent.replay(agent.impressions)\n",
    "    \n",
    "    #update q targets weights\n",
    "    #+1 made for not saving since start\n",
    "    if (i+1) % 15 == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    #save model every 5 episodes\n",
    "    if (i+1) % 5 == 0:\n",
    "        agent.save_network(path)\n",
    "         \n",
    "    #for plot\n",
    "    total_episode_rewards.append(game.get_total_reward())\n",
    "    timestamps.append(timestamp)\n",
    "    timestamp = timestamp + 1\n",
    "    \n",
    "    print (\"Result:\", game.get_total_reward())\n",
    "    time.sleep(4)\n",
    "game.close()\n",
    "agent.save_network(path)\n",
    "agent.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_network(path)\n",
    "#agent.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.explore_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_network(path)\n",
    "#agent.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timestamps, total_episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent will use model to act\n",
    "agent.explore_start = 0.01\n",
    "\n",
    "#games and steps in 1 episode\n",
    "episodes = 1500\n",
    "max_steps = 100 \n",
    "\n",
    "game.init()\n",
    "#game_rewards = []\n",
    "#game_timestamps = []\n",
    "#timestamp = 1\n",
    "\n",
    "for i in range(episodes):\n",
    "    print('Episode ' + str(i))\n",
    "    game.new_episode()\n",
    "    \n",
    "    finished = game.is_episode_finished()\n",
    "    if finished:\n",
    "        print('Finished? - ' + str(finished))\n",
    "        game.close()\n",
    "        game.init()\n",
    "        game.new_episode()\n",
    "    \n",
    "    episode_rewards = []\n",
    "    step = 0\n",
    "    while step < max_steps:\n",
    "        if game.is_new_episode():\n",
    "            state = game.get_state().screen_buffer\n",
    "            test_state = state\n",
    "            agent.frame = agent.make_frame(state)\n",
    "            state = agent.frame\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            episode_rewards.append(reward)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            step = step + 1\n",
    "            time.sleep(0.02)\n",
    "        #все остальные фреймы и до конца\n",
    "        else:\n",
    "            state = game.get_state().screen_buffer\n",
    "            agent.frame = agent.make_frame(state)\n",
    "            state = agent.frame\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            reward = game.make_action(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                step = max_steps\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "                time.sleep(0.02)\n",
    "                break\n",
    "            else:\n",
    "                step = step + 1\n",
    "                if step == 199:\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                time.sleep(0.02)        \n",
    "    print (\"Result:\", game.get_total_reward())\n",
    "    time.sleep(2)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=/Users/s.matrosov/Downloads/Doom_Model/3/policy_gradients/\n",
    "%tensorboard --logdir '/Users/s.matrosov/Downloads/Doom_Model/3/policy_gradients/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
